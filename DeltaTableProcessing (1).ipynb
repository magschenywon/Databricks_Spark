{"cells":[{"cell_type":"code","source":["%python\n # File location and type\n file_location = \"/mnt/files/Employee.csv\"\n file_type = \"csv\"\n \n # CSV options\n infer_schema = \"false\"\n first_row_is_header = \"true\"\n delimiter = \",\"\n \n # The applied options are for CSV files. For other file types, these will be ignored.\n df = spark.read.format(file_type) \\\n   .option(\"inferSchema\", infer_schema) \\\n   .option(\"header\", first_row_is_header) \\\n   .option(\"sep\", delimiter) \\\n   .load(file_location)\n \n #display(df)\n \n temp_table_name = \"employee_csv\"\n df.createOrReplaceTempView(temp_table_name)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f48dbe43-0e99-4bb0-9bb6-200c280ff10f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"Employee_id","nullable":true,"type":"string"},{"metadata":{},"name":"First_Name","nullable":true,"type":"string"},{"metadata":{},"name":"Last_Name","nullable":true,"type":"string"},{"metadata":{},"name":"Gender","nullable":true,"type":"string"},{"metadata":{},"name":"Salary","nullable":true,"type":"string"},{"metadata":{},"name":"Date_of_Birth","nullable":true,"type":"string"},{"metadata":{},"name":"Age","nullable":true,"type":"string"},{"metadata":{},"name":"Country","nullable":true,"type":"string"},{"metadata":{},"name":"Department_id","nullable":true,"type":"string"},{"metadata":{},"name":"Date_of_Joining","nullable":true,"type":"string"},{"metadata":{},"name":"Manager_id","nullable":true,"type":"string"},{"metadata":{},"name":"Currency","nullable":true,"type":"string"},{"metadata":{},"name":"End_Date","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nselect count(*) from employee_csv"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a668f753-0d55-460c-8f5f-b30bc6a33e25"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[{"name":"_sqldf","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"count(1)","nullable":false,"type":"long"}],"type":"struct"},"tableIdentifier":null}],"data":[[1000]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"count(1)","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>count(1)</th></tr></thead><tbody><tr><td>1000</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%python\n# File location and type\nfile_location = \"/mnt/files/upsert_data.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"false\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\ndisplay(df)\n\n# Create a view or table\n\ntemp_table_name = \"upsert_data_csv\"\n\ndf.createOrReplaceTempView(temp_table_name)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7b6ed565-1cfa-41fe-b275-683f3a863345"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[{"name":"df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"Employee_id","nullable":true,"type":"string"},{"metadata":{},"name":"First_Name","nullable":true,"type":"string"},{"metadata":{},"name":"Last_Name","nullable":true,"type":"string"},{"metadata":{},"name":"Gender","nullable":true,"type":"string"},{"metadata":{},"name":"Salary","nullable":true,"type":"string"},{"metadata":{},"name":"Date_of_Birth","nullable":true,"type":"string"},{"metadata":{},"name":"Age","nullable":true,"type":"string"},{"metadata":{},"name":"Country","nullable":true,"type":"string"},{"metadata":{},"name":"Department_id","nullable":true,"type":"string"},{"metadata":{},"name":"Date_of_Joining","nullable":true,"type":"string"},{"metadata":{},"name":"Manager_id","nullable":true,"type":"string"},{"metadata":{},"name":"Currency","nullable":true,"type":"string"},{"metadata":{},"name":"End_Date","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null}],"data":[["1001","Liam","Nassie","Male","105623","07/11/1988","60","Sweden","4","23/10/2014","7","INR","30/11/2020"],["2","Kristopher","Farr","Female","69531","29/07/1981","13","China","10","28/05/2016","7","AUD","09/09/2018"],["3","Kellina","Singh","Female","96879","24/07/1984","27","India","12","21/03/2016","9","INR","09/04/2020"],["4","Tobit","Patel","Female","76070","29/11/1981","44","India","4","12/11/2012","17","INR","04/10/2019"],["1002","Noah","Bampkin","Male","62228","03/04/1981","54","USA","7","14/07/2013","8","AUD","20/10/2018"],["1003","Oliver","Caser","Male","106837","27/05/1984","14","China","5","15/10/2014","5","INR","03/04/2020"],["1004","William","Dooler","Male","77729","03/10/1991","-12","Sweden","17","21/06/2014","9","AUD","24/02/2018"],["1005","Elijah","Mainson","Male","81590","05/05/1992","60","Sweden","7","14/04/2018","18","INR","29/03/2019"],["1006","James","Huckle","Male","71206","08/04/1983","-10","Germany","13","15/05/2018","19","AUD","30/12/2020"],["1007","Benjamin","Tipens","Male","98642","21/03/1986","53","USA","11","31/03/2017","2","INR","22/11/2020"],["1008","Joseph","Choulerton","Male","108740","29/01/1985","40","Australia","9","28/09/2013","13","INR","23/08/2020"],["12","Kathy","Singh","Female","121493","17/10/1991","-9","Brazil","8","17/09/2010","8","INR","02/10/2020"],["1009","Matthew","Banks","Male","67525","26/08/1985","-6","China","8","08/12/2011","17","INR","28/12/2019"],["14","Loni","Nagy","Female","119141","27/01/1990","-2","Canada","8","14/10/2017","19","INR","22/05/2018"],["15","Melitta","Patel","Female","110615","24/08/1987","13","Germany","11","22/01/2018","20","INR","13/05/2018"],["1010","Jaxon","Lurcock","Male","110210","26/03/1980","14","Denmark","7","21/09/2017","7","AUD","04/11/2020"],["17","Ben","Patel","Female","126559","01/07/1982","28","China","7","29/12/2018","12","AUD","30/03/2018"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"Employee_id","type":"\"string\"","metadata":"{}"},{"name":"First_Name","type":"\"string\"","metadata":"{}"},{"name":"Last_Name","type":"\"string\"","metadata":"{}"},{"name":"Gender","type":"\"string\"","metadata":"{}"},{"name":"Salary","type":"\"string\"","metadata":"{}"},{"name":"Date_of_Birth","type":"\"string\"","metadata":"{}"},{"name":"Age","type":"\"string\"","metadata":"{}"},{"name":"Country","type":"\"string\"","metadata":"{}"},{"name":"Department_id","type":"\"string\"","metadata":"{}"},{"name":"Date_of_Joining","type":"\"string\"","metadata":"{}"},{"name":"Manager_id","type":"\"string\"","metadata":"{}"},{"name":"Currency","type":"\"string\"","metadata":"{}"},{"name":"End_Date","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Employee_id</th><th>First_Name</th><th>Last_Name</th><th>Gender</th><th>Salary</th><th>Date_of_Birth</th><th>Age</th><th>Country</th><th>Department_id</th><th>Date_of_Joining</th><th>Manager_id</th><th>Currency</th><th>End_Date</th></tr></thead><tbody><tr><td>1001</td><td>Liam</td><td>Nassie</td><td>Male</td><td>105623</td><td>07/11/1988</td><td>60</td><td>Sweden</td><td>4</td><td>23/10/2014</td><td>7</td><td>INR</td><td>30/11/2020</td></tr><tr><td>2</td><td>Kristopher</td><td>Farr</td><td>Female</td><td>69531</td><td>29/07/1981</td><td>13</td><td>China</td><td>10</td><td>28/05/2016</td><td>7</td><td>AUD</td><td>09/09/2018</td></tr><tr><td>3</td><td>Kellina</td><td>Singh</td><td>Female</td><td>96879</td><td>24/07/1984</td><td>27</td><td>India</td><td>12</td><td>21/03/2016</td><td>9</td><td>INR</td><td>09/04/2020</td></tr><tr><td>4</td><td>Tobit</td><td>Patel</td><td>Female</td><td>76070</td><td>29/11/1981</td><td>44</td><td>India</td><td>4</td><td>12/11/2012</td><td>17</td><td>INR</td><td>04/10/2019</td></tr><tr><td>1002</td><td>Noah</td><td>Bampkin</td><td>Male</td><td>62228</td><td>03/04/1981</td><td>54</td><td>USA</td><td>7</td><td>14/07/2013</td><td>8</td><td>AUD</td><td>20/10/2018</td></tr><tr><td>1003</td><td>Oliver</td><td>Caser</td><td>Male</td><td>106837</td><td>27/05/1984</td><td>14</td><td>China</td><td>5</td><td>15/10/2014</td><td>5</td><td>INR</td><td>03/04/2020</td></tr><tr><td>1004</td><td>William</td><td>Dooler</td><td>Male</td><td>77729</td><td>03/10/1991</td><td>-12</td><td>Sweden</td><td>17</td><td>21/06/2014</td><td>9</td><td>AUD</td><td>24/02/2018</td></tr><tr><td>1005</td><td>Elijah</td><td>Mainson</td><td>Male</td><td>81590</td><td>05/05/1992</td><td>60</td><td>Sweden</td><td>7</td><td>14/04/2018</td><td>18</td><td>INR</td><td>29/03/2019</td></tr><tr><td>1006</td><td>James</td><td>Huckle</td><td>Male</td><td>71206</td><td>08/04/1983</td><td>-10</td><td>Germany</td><td>13</td><td>15/05/2018</td><td>19</td><td>AUD</td><td>30/12/2020</td></tr><tr><td>1007</td><td>Benjamin</td><td>Tipens</td><td>Male</td><td>98642</td><td>21/03/1986</td><td>53</td><td>USA</td><td>11</td><td>31/03/2017</td><td>2</td><td>INR</td><td>22/11/2020</td></tr><tr><td>1008</td><td>Joseph</td><td>Choulerton</td><td>Male</td><td>108740</td><td>29/01/1985</td><td>40</td><td>Australia</td><td>9</td><td>28/09/2013</td><td>13</td><td>INR</td><td>23/08/2020</td></tr><tr><td>12</td><td>Kathy</td><td>Singh</td><td>Female</td><td>121493</td><td>17/10/1991</td><td>-9</td><td>Brazil</td><td>8</td><td>17/09/2010</td><td>8</td><td>INR</td><td>02/10/2020</td></tr><tr><td>1009</td><td>Matthew</td><td>Banks</td><td>Male</td><td>67525</td><td>26/08/1985</td><td>-6</td><td>China</td><td>8</td><td>08/12/2011</td><td>17</td><td>INR</td><td>28/12/2019</td></tr><tr><td>14</td><td>Loni</td><td>Nagy</td><td>Female</td><td>119141</td><td>27/01/1990</td><td>-2</td><td>Canada</td><td>8</td><td>14/10/2017</td><td>19</td><td>INR</td><td>22/05/2018</td></tr><tr><td>15</td><td>Melitta</td><td>Patel</td><td>Female</td><td>110615</td><td>24/08/1987</td><td>13</td><td>Germany</td><td>11</td><td>22/01/2018</td><td>20</td><td>INR</td><td>13/05/2018</td></tr><tr><td>1010</td><td>Jaxon</td><td>Lurcock</td><td>Male</td><td>110210</td><td>26/03/1980</td><td>14</td><td>Denmark</td><td>7</td><td>21/09/2017</td><td>7</td><td>AUD</td><td>04/11/2020</td></tr><tr><td>17</td><td>Ben</td><td>Patel</td><td>Female</td><td>126559</td><td>01/07/1982</td><td>28</td><td>China</td><td>7</td><td>29/12/2018</td><td>12</td><td>AUD</td><td>30/03/2018</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nselect count(*) from upsert_data_csv"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ea80f2b1-1fb6-4b32-bb19-b1320d8c9c37"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[{"name":"_sqldf","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"count(1)","nullable":false,"type":"long"}],"type":"struct"},"tableIdentifier":null}],"data":[[17]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"count(1)","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>count(1)</th></tr></thead><tbody><tr><td>17</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nCREATE DATABASE IF NOT EXISTS DATAVOWELDB;\nUSE DATAVOWELDB"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"64114271-3e5d-472d-a71e-d894ef9bba2f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[{"name":"_sqldf","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[],"type":"struct"},"tableIdentifier":null}],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\n--LOCATION is where we want to store out data table\n--from EMPLOYEE_CSV insert all data which the department_id is not null to this new data table\nDROP TABLE IF EXISTS EMPLOYEE_DELTA_TABLE;\n\nCREATE TABLE EMPLOYEE_DELTA_TABLE\nUSING delta\nPARTITIONED BY (DEPARTMENT_ID)\nLOCATION \"/DATAVOWEL/DELTA/EMPLOYEE_DATA\"\nAS \n  (\n  SELECT * FROM EMPLOYEE_CSV WHERE DEPARTMENT_ID IS NOT NULL\n  )\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5158ced9-c45a-4d69-b966-d73e5e7e245b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[{"name":"_sqldf","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[],"type":"struct"},"tableIdentifier":null}],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nDESCRIBE DETAIL EMPLOYEE_DELTA_TABLE"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0284da59-42ed-48bb-8531-29f0eabfb3f6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[{"name":"_sqldf","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"format","nullable":true,"type":"string"},{"metadata":{},"name":"id","nullable":true,"type":"string"},{"metadata":{},"name":"name","nullable":true,"type":"string"},{"metadata":{},"name":"description","nullable":true,"type":"string"},{"metadata":{},"name":"location","nullable":true,"type":"string"},{"metadata":{},"name":"createdAt","nullable":true,"type":"timestamp"},{"metadata":{},"name":"lastModified","nullable":true,"type":"timestamp"},{"metadata":{},"name":"partitionColumns","nullable":true,"type":{"containsNull":true,"elementType":"string","type":"array"}},{"metadata":{},"name":"numFiles","nullable":true,"type":"long"},{"metadata":{},"name":"sizeInBytes","nullable":true,"type":"long"},{"metadata":{},"name":"properties","nullable":true,"type":{"keyType":"string","type":"map","valueContainsNull":true,"valueType":"string"}},{"metadata":{},"name":"minReaderVersion","nullable":true,"type":"integer"},{"metadata":{},"name":"minWriterVersion","nullable":true,"type":"integer"}],"type":"struct"},"tableIdentifier":null}],"data":[["delta","ba0c0808-7961-4e58-8f08-a094a784fb14","datavoweldb.employee_delta_table",null,"dbfs:/DATAVOWEL/DELTA/EMPLOYEE_DATA","2022-11-03T23:17:08.421+0000","2022-11-03T23:17:15.000+0000",["Department_id"],20,119264,{},1,2]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"format","type":"\"string\"","metadata":"{}"},{"name":"id","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"description","type":"\"string\"","metadata":"{}"},{"name":"location","type":"\"string\"","metadata":"{}"},{"name":"createdAt","type":"\"timestamp\"","metadata":"{}"},{"name":"lastModified","type":"\"timestamp\"","metadata":"{}"},{"name":"partitionColumns","type":"{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}","metadata":"{}"},{"name":"numFiles","type":"\"long\"","metadata":"{}"},{"name":"sizeInBytes","type":"\"long\"","metadata":"{}"},{"name":"properties","type":"{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}","metadata":"{}"},{"name":"minReaderVersion","type":"\"integer\"","metadata":"{}"},{"name":"minWriterVersion","type":"\"integer\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>format</th><th>id</th><th>name</th><th>description</th><th>location</th><th>createdAt</th><th>lastModified</th><th>partitionColumns</th><th>numFiles</th><th>sizeInBytes</th><th>properties</th><th>minReaderVersion</th><th>minWriterVersion</th></tr></thead><tbody><tr><td>delta</td><td>ba0c0808-7961-4e58-8f08-a094a784fb14</td><td>datavoweldb.employee_delta_table</td><td>null</td><td>dbfs:/DATAVOWEL/DELTA/EMPLOYEE_DATA</td><td>2022-11-03T23:17:08.421+0000</td><td>2022-11-03T23:17:15.000+0000</td><td>List(Department_id)</td><td>20</td><td>119264</td><td>Map()</td><td>1</td><td>2</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nselect count(*) from EMPLOYEE_DELTA_TABLE; --988\nselect count(*) from UPSERT_DATA_CSV;  --17"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b039025d-a4ce-49a8-b383-2d25ae85a56f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[{"name":"_sqldf","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"count(1)","nullable":false,"type":"long"}],"type":"struct"},"tableIdentifier":null}],"data":[[17]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"count(1)","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>count(1)</th></tr></thead><tbody><tr><td>17</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\n-- IN EMPLOYEE_DELTA_TABLE THERE ARE 988 ROWS\n--AFTER DOING MERGE OPERATION 998 ROWS WILL BE THERE AND 7 ROWS WILL BE UPDATED\nMERGE INTO EMPLOYEE_DELTA_TABLE                            -- the MERGE instruction is used to perform the upsert\nUSING upsert_data_csv\n\nON EMPLOYEE_DELTA_TABLE.employee_id = upsert_data_csv.employee_id -- ON is used to describe the MERGE condition\n   \nWHEN MATCHED THEN                                           -- WHEN MATCHED describes the update behavior\n  UPDATE SET\n  EMPLOYEE_DELTA_TABLE.Last_Name = upsert_data_csv.Last_Name   \nWHEN NOT MATCHED THEN                                       -- WHEN NOT MATCHED describes the insert behavior\n  INSERT (Employee_id,First_Name,Last_Name,Gender,Salary,Date_of_Birth,Age,Country,Department_id,Date_of_Joining,Manager_id,Currency,End_Date)              \n  VALUES (Employee_id,First_Name,Last_Name,Gender,Salary,Date_of_Birth,Age,Country,Department_id,Date_of_Joining,Manager_id,Currency,End_Date)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a8f72a4-e34f-4c47-84fc-387ab3c8f9c7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[{"name":"_sqldf","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[],"type":"struct"},"tableIdentifier":null}],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT COUNT(*) FROM EMPLOYEE_DELTA_TABLE"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f4e59cac-3819-4087-98bb-fe1506eaffa0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[{"name":"_sqldf","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"count(1)","nullable":false,"type":"long"}],"type":"struct"},"tableIdentifier":null}],"data":[[998]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"count(1)","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>count(1)</th></tr></thead><tbody><tr><td>998</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nDESCRIBE HISTORY EMPLOYEE_DELTA_TABLE"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8047a043-a3f1-4847-a431-49c6b77541c9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[{"name":"_sqldf","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"version","nullable":true,"type":"long"},{"metadata":{},"name":"timestamp","nullable":true,"type":"timestamp"},{"metadata":{},"name":"userId","nullable":true,"type":"string"},{"metadata":{},"name":"userName","nullable":true,"type":"string"},{"metadata":{},"name":"operation","nullable":true,"type":"string"},{"metadata":{},"name":"operationParameters","nullable":true,"type":{"keyType":"string","type":"map","valueContainsNull":true,"valueType":"string"}},{"metadata":{},"name":"job","nullable":true,"type":{"fields":[{"metadata":{},"name":"jobId","nullable":true,"type":"string"},{"metadata":{},"name":"jobName","nullable":true,"type":"string"},{"metadata":{},"name":"runId","nullable":true,"type":"string"},{"metadata":{},"name":"jobOwnerId","nullable":true,"type":"string"},{"metadata":{},"name":"triggerType","nullable":true,"type":"string"}],"type":"struct"}},{"metadata":{},"name":"notebook","nullable":true,"type":{"fields":[{"metadata":{},"name":"notebookId","nullable":true,"type":"string"}],"type":"struct"}},{"metadata":{},"name":"clusterId","nullable":true,"type":"string"},{"metadata":{},"name":"readVersion","nullable":true,"type":"long"},{"metadata":{},"name":"isolationLevel","nullable":true,"type":"string"},{"metadata":{},"name":"isBlindAppend","nullable":true,"type":"boolean"},{"metadata":{},"name":"operationMetrics","nullable":true,"type":{"keyType":"string","type":"map","valueContainsNull":true,"valueType":"string"}},{"metadata":{},"name":"userMetadata","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null}],"data":[[1,"2022-11-03T23:24:30.000+0000","8645928144849819","magschenywon@gmail.com","MERGE",{"predicate":"(spark_catalog.datavoweldb.EMPLOYEE_DELTA_TABLE.`employee_id` = upsert_data_csv.`employee_id`)","matchedPredicates":"[{\"actionType\":\"update\"}]","notMatchedPredicates":"[{\"actionType\":\"insert\"}]"},null,["4125592599887365"],"1103-151030-blo4h8jy",0,"WriteSerializable",false,{"numTargetRowsCopied":"274","numTargetRowsDeleted":"0","numTargetFilesAdded":"10","numTargetRowsInserted":"10","numTargetRowsUpdated":"7","numOutputRows":"291","numSourceRows":"17","numTargetFilesRemoved":"6"},null],[0,"2022-11-03T23:17:15.000+0000","8645928144849819","magschenywon@gmail.com","CREATE TABLE AS SELECT",{"isManaged":"false","description":null,"partitionBy":"[\"Department_id\"]","properties":"{}"},null,["4125592599887365"],"1103-151030-blo4h8jy",null,"WriteSerializable",true,{"numFiles":"20","numOutputRows":"988","numOutputBytes":"119264"},null]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"version","type":"\"long\"","metadata":"{}"},{"name":"timestamp","type":"\"timestamp\"","metadata":"{}"},{"name":"userId","type":"\"string\"","metadata":"{}"},{"name":"userName","type":"\"string\"","metadata":"{}"},{"name":"operation","type":"\"string\"","metadata":"{}"},{"name":"operationParameters","type":"{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}","metadata":"{}"},{"name":"job","type":"{\"type\":\"struct\",\"fields\":[{\"name\":\"jobId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"runId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobOwnerId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"triggerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}","metadata":"{}"},{"name":"notebook","type":"{\"type\":\"struct\",\"fields\":[{\"name\":\"notebookId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}","metadata":"{}"},{"name":"clusterId","type":"\"string\"","metadata":"{}"},{"name":"readVersion","type":"\"long\"","metadata":"{}"},{"name":"isolationLevel","type":"\"string\"","metadata":"{}"},{"name":"isBlindAppend","type":"\"boolean\"","metadata":"{}"},{"name":"operationMetrics","type":"{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}","metadata":"{}"},{"name":"userMetadata","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th></tr></thead><tbody><tr><td>1</td><td>2022-11-03T23:24:30.000+0000</td><td>8645928144849819</td><td>magschenywon@gmail.com</td><td>MERGE</td><td>Map(predicate -> (spark_catalog.datavoweldb.EMPLOYEE_DELTA_TABLE.`employee_id` = upsert_data_csv.`employee_id`), matchedPredicates -> [{\"actionType\":\"update\"}], notMatchedPredicates -> [{\"actionType\":\"insert\"}])</td><td>null</td><td>List(4125592599887365)</td><td>1103-151030-blo4h8jy</td><td>0</td><td>WriteSerializable</td><td>false</td><td>Map(numTargetRowsCopied -> 274, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 10, numTargetRowsInserted -> 10, numTargetRowsUpdated -> 7, numOutputRows -> 291, numSourceRows -> 17, numTargetFilesRemoved -> 6)</td><td>null</td></tr><tr><td>0</td><td>2022-11-03T23:17:15.000+0000</td><td>8645928144849819</td><td>magschenywon@gmail.com</td><td>CREATE TABLE AS SELECT</td><td>Map(isManaged -> false, description -> null, partitionBy -> [\"Department_id\"], properties -> {})</td><td>null</td><td>List(4125592599887365)</td><td>1103-151030-blo4h8jy</td><td>null</td><td>WriteSerializable</td><td>true</td><td>Map(numFiles -> 20, numOutputRows -> 988, numOutputBytes -> 119264)</td><td>null</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\n--check the previous version data when employee_id = 12\nSELECT * FROM EMPLOYEE_DELTA_TABLE VERSION AS OF 0 WHERE EMPLOYEE_ID = 12"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ff42bb4f-10c9-461b-b86e-1801b4eea9b3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[{"name":"_sqldf","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"Employee_id","nullable":true,"type":"string"},{"metadata":{},"name":"First_Name","nullable":true,"type":"string"},{"metadata":{},"name":"Last_Name","nullable":true,"type":"string"},{"metadata":{},"name":"Gender","nullable":true,"type":"string"},{"metadata":{},"name":"Salary","nullable":true,"type":"string"},{"metadata":{},"name":"Date_of_Birth","nullable":true,"type":"string"},{"metadata":{},"name":"Age","nullable":true,"type":"string"},{"metadata":{},"name":"Country","nullable":true,"type":"string"},{"metadata":{},"name":"Department_id","nullable":true,"type":"string"},{"metadata":{},"name":"Date_of_Joining","nullable":true,"type":"string"},{"metadata":{},"name":"Manager_id","nullable":true,"type":"string"},{"metadata":{},"name":"Currency","nullable":true,"type":"string"},{"metadata":{},"name":"End_Date","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null}],"data":[["12","Kathy","Milstead","Female","121493","17/10/1991","-9","Brazil","8","17/09/2010","8","INR","02/10/2020"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"Employee_id","type":"\"string\"","metadata":"{}"},{"name":"First_Name","type":"\"string\"","metadata":"{}"},{"name":"Last_Name","type":"\"string\"","metadata":"{}"},{"name":"Gender","type":"\"string\"","metadata":"{}"},{"name":"Salary","type":"\"string\"","metadata":"{}"},{"name":"Date_of_Birth","type":"\"string\"","metadata":"{}"},{"name":"Age","type":"\"string\"","metadata":"{}"},{"name":"Country","type":"\"string\"","metadata":"{}"},{"name":"Department_id","type":"\"string\"","metadata":"{}"},{"name":"Date_of_Joining","type":"\"string\"","metadata":"{}"},{"name":"Manager_id","type":"\"string\"","metadata":"{}"},{"name":"Currency","type":"\"string\"","metadata":"{}"},{"name":"End_Date","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Employee_id</th><th>First_Name</th><th>Last_Name</th><th>Gender</th><th>Salary</th><th>Date_of_Birth</th><th>Age</th><th>Country</th><th>Department_id</th><th>Date_of_Joining</th><th>Manager_id</th><th>Currency</th><th>End_Date</th></tr></thead><tbody><tr><td>12</td><td>Kathy</td><td>Milstead</td><td>Female</td><td>121493</td><td>17/10/1991</td><td>-9</td><td>Brazil</td><td>8</td><td>17/09/2010</td><td>8</td><td>INR</td><td>02/10/2020</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\n--check the updated version data when employee_id = 12\nSELECT * FROM EMPLOYEE_DELTA_TABLE VERSION AS OF 1 WHERE EMPLOYEE_ID = 12"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d3c419dd-aa6b-4425-a4c3-891c2983edd8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[{"name":"_sqldf","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"Employee_id","nullable":true,"type":"string"},{"metadata":{},"name":"First_Name","nullable":true,"type":"string"},{"metadata":{},"name":"Last_Name","nullable":true,"type":"string"},{"metadata":{},"name":"Gender","nullable":true,"type":"string"},{"metadata":{},"name":"Salary","nullable":true,"type":"string"},{"metadata":{},"name":"Date_of_Birth","nullable":true,"type":"string"},{"metadata":{},"name":"Age","nullable":true,"type":"string"},{"metadata":{},"name":"Country","nullable":true,"type":"string"},{"metadata":{},"name":"Department_id","nullable":true,"type":"string"},{"metadata":{},"name":"Date_of_Joining","nullable":true,"type":"string"},{"metadata":{},"name":"Manager_id","nullable":true,"type":"string"},{"metadata":{},"name":"Currency","nullable":true,"type":"string"},{"metadata":{},"name":"End_Date","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null}],"data":[["12","Kathy","Singh","Female","121493","17/10/1991","-9","Brazil","8","17/09/2010","8","INR","02/10/2020"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"Employee_id","type":"\"string\"","metadata":"{}"},{"name":"First_Name","type":"\"string\"","metadata":"{}"},{"name":"Last_Name","type":"\"string\"","metadata":"{}"},{"name":"Gender","type":"\"string\"","metadata":"{}"},{"name":"Salary","type":"\"string\"","metadata":"{}"},{"name":"Date_of_Birth","type":"\"string\"","metadata":"{}"},{"name":"Age","type":"\"string\"","metadata":"{}"},{"name":"Country","type":"\"string\"","metadata":"{}"},{"name":"Department_id","type":"\"string\"","metadata":"{}"},{"name":"Date_of_Joining","type":"\"string\"","metadata":"{}"},{"name":"Manager_id","type":"\"string\"","metadata":"{}"},{"name":"Currency","type":"\"string\"","metadata":"{}"},{"name":"End_Date","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Employee_id</th><th>First_Name</th><th>Last_Name</th><th>Gender</th><th>Salary</th><th>Date_of_Birth</th><th>Age</th><th>Country</th><th>Department_id</th><th>Date_of_Joining</th><th>Manager_id</th><th>Currency</th><th>End_Date</th></tr></thead><tbody><tr><td>12</td><td>Kathy</td><td>Singh</td><td>Female</td><td>121493</td><td>17/10/1991</td><td>-9</td><td>Brazil</td><td>8</td><td>17/09/2010</td><td>8</td><td>INR</td><td>02/10/2020</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%fs ls /DATAVOWEL/DELTA/EMPLOYEE_DATA/_delta_log"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f3e8b74-6b23-4df7-9ac0-61bfbd3493ea"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/DATAVOWEL/DELTA/EMPLOYEE_DATA/_delta_log/00000000000000000000.crc","00000000000000000000.crc",92],["dbfs:/DATAVOWEL/DELTA/EMPLOYEE_DATA/_delta_log/00000000000000000000.json","00000000000000000000.json",23058],["dbfs:/DATAVOWEL/DELTA/EMPLOYEE_DATA/_delta_log/00000000000000000001.crc","00000000000000000001.crc",92],["dbfs:/DATAVOWEL/DELTA/EMPLOYEE_DATA/_delta_log/00000000000000000001.json","00000000000000000001.json",12351],["dbfs:/DATAVOWEL/DELTA/EMPLOYEE_DATA/_delta_log/__tmp_path_dir/","__tmp_path_dir/",0]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/DATAVOWEL/DELTA/EMPLOYEE_DATA/_delta_log/00000000000000000000.crc</td><td>00000000000000000000.crc</td><td>92</td></tr><tr><td>dbfs:/DATAVOWEL/DELTA/EMPLOYEE_DATA/_delta_log/00000000000000000000.json</td><td>00000000000000000000.json</td><td>23058</td></tr><tr><td>dbfs:/DATAVOWEL/DELTA/EMPLOYEE_DATA/_delta_log/00000000000000000001.crc</td><td>00000000000000000001.crc</td><td>92</td></tr><tr><td>dbfs:/DATAVOWEL/DELTA/EMPLOYEE_DATA/_delta_log/00000000000000000001.json</td><td>00000000000000000001.json</td><td>12351</td></tr><tr><td>dbfs:/DATAVOWEL/DELTA/EMPLOYEE_DATA/_delta_log/__tmp_path_dir/</td><td>__tmp_path_dir/</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nselect * from employee_delta_table where employee_id in (21,35,45,47,49)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3023a459-2446-47d9-8ba1-435b09aea8f1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[{"name":"_sqldf","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"Employee_id","nullable":true,"type":"string"},{"metadata":{},"name":"First_Name","nullable":true,"type":"string"},{"metadata":{},"name":"Last_Name","nullable":true,"type":"string"},{"metadata":{},"name":"Gender","nullable":true,"type":"string"},{"metadata":{},"name":"Salary","nullable":true,"type":"string"},{"metadata":{},"name":"Date_of_Birth","nullable":true,"type":"string"},{"metadata":{},"name":"Age","nullable":true,"type":"string"},{"metadata":{},"name":"Country","nullable":true,"type":"string"},{"metadata":{},"name":"Department_id","nullable":true,"type":"string"},{"metadata":{},"name":"Date_of_Joining","nullable":true,"type":"string"},{"metadata":{},"name":"Manager_id","nullable":true,"type":"string"},{"metadata":{},"name":"Currency","nullable":true,"type":"string"},{"metadata":{},"name":"End_Date","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null}],"data":[["21","Sherie","Onele","Female","96185","05/11/1990","-10","China","16","26/01/2014","9","AUD","18/12/2020"],["35","Gabriel","Ormshaw","Female","120118","06/10/1984","25","USA","16","21/09/2015","19","INR","07/01/2020"],["45","Devin","Passler","Female","77288","12/10/1982","-10","Japan","16","09/03/2013","17","INR","31/08/2019"],["47","Bart","Astbery","Male","64429","06/01/1992","52","Japan","16","11/06/2016","1","AUD","26/03/2020"],["49","Vikky","McCloch","Male","95866","04/04/1989","41","USA","16","13/05/2014","17","INR","21/03/2020"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"Employee_id","type":"\"string\"","metadata":"{}"},{"name":"First_Name","type":"\"string\"","metadata":"{}"},{"name":"Last_Name","type":"\"string\"","metadata":"{}"},{"name":"Gender","type":"\"string\"","metadata":"{}"},{"name":"Salary","type":"\"string\"","metadata":"{}"},{"name":"Date_of_Birth","type":"\"string\"","metadata":"{}"},{"name":"Age","type":"\"string\"","metadata":"{}"},{"name":"Country","type":"\"string\"","metadata":"{}"},{"name":"Department_id","type":"\"string\"","metadata":"{}"},{"name":"Date_of_Joining","type":"\"string\"","metadata":"{}"},{"name":"Manager_id","type":"\"string\"","metadata":"{}"},{"name":"Currency","type":"\"string\"","metadata":"{}"},{"name":"End_Date","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Employee_id</th><th>First_Name</th><th>Last_Name</th><th>Gender</th><th>Salary</th><th>Date_of_Birth</th><th>Age</th><th>Country</th><th>Department_id</th><th>Date_of_Joining</th><th>Manager_id</th><th>Currency</th><th>End_Date</th></tr></thead><tbody><tr><td>21</td><td>Sherie</td><td>Onele</td><td>Female</td><td>96185</td><td>05/11/1990</td><td>-10</td><td>China</td><td>16</td><td>26/01/2014</td><td>9</td><td>AUD</td><td>18/12/2020</td></tr><tr><td>35</td><td>Gabriel</td><td>Ormshaw</td><td>Female</td><td>120118</td><td>06/10/1984</td><td>25</td><td>USA</td><td>16</td><td>21/09/2015</td><td>19</td><td>INR</td><td>07/01/2020</td></tr><tr><td>45</td><td>Devin</td><td>Passler</td><td>Female</td><td>77288</td><td>12/10/1982</td><td>-10</td><td>Japan</td><td>16</td><td>09/03/2013</td><td>17</td><td>INR</td><td>31/08/2019</td></tr><tr><td>47</td><td>Bart</td><td>Astbery</td><td>Male</td><td>64429</td><td>06/01/1992</td><td>52</td><td>Japan</td><td>16</td><td>11/06/2016</td><td>1</td><td>AUD</td><td>26/03/2020</td></tr><tr><td>49</td><td>Vikky</td><td>McCloch</td><td>Male</td><td>95866</td><td>04/04/1989</td><td>41</td><td>USA</td><td>16</td><td>13/05/2014</td><td>17</td><td>INR</td><td>21/03/2020</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\n--update the table by setting first_name to null for above employee_id\nUPDATE datavoweldb.EMPLOYEE_DELTA_TABLE SET First_Name = Null where employee_id in (21,35,45,47,49) "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"da224ad7-073f-46e4-b7dd-02d13c5d5cd7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[{"name":"_sqldf","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[],"type":"struct"},"tableIdentifier":null}],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nselect * from employee_delta_table where employee_id in (21,35,45,47,49)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"68d85a59-614c-44d8-9994-fbe7013d51ef"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[{"name":"_sqldf","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"Employee_id","nullable":true,"type":"string"},{"metadata":{},"name":"First_Name","nullable":true,"type":"string"},{"metadata":{},"name":"Last_Name","nullable":true,"type":"string"},{"metadata":{},"name":"Gender","nullable":true,"type":"string"},{"metadata":{},"name":"Salary","nullable":true,"type":"string"},{"metadata":{},"name":"Date_of_Birth","nullable":true,"type":"string"},{"metadata":{},"name":"Age","nullable":true,"type":"string"},{"metadata":{},"name":"Country","nullable":true,"type":"string"},{"metadata":{},"name":"Department_id","nullable":true,"type":"string"},{"metadata":{},"name":"Date_of_Joining","nullable":true,"type":"string"},{"metadata":{},"name":"Manager_id","nullable":true,"type":"string"},{"metadata":{},"name":"Currency","nullable":true,"type":"string"},{"metadata":{},"name":"End_Date","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null}],"data":[["21",null,"Onele","Female","96185","05/11/1990","-10","China","16","26/01/2014","9","AUD","18/12/2020"],["35",null,"Ormshaw","Female","120118","06/10/1984","25","USA","16","21/09/2015","19","INR","07/01/2020"],["45",null,"Passler","Female","77288","12/10/1982","-10","Japan","16","09/03/2013","17","INR","31/08/2019"],["47",null,"Astbery","Male","64429","06/01/1992","52","Japan","16","11/06/2016","1","AUD","26/03/2020"],["49",null,"McCloch","Male","95866","04/04/1989","41","USA","16","13/05/2014","17","INR","21/03/2020"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"Employee_id","type":"\"string\"","metadata":"{}"},{"name":"First_Name","type":"\"string\"","metadata":"{}"},{"name":"Last_Name","type":"\"string\"","metadata":"{}"},{"name":"Gender","type":"\"string\"","metadata":"{}"},{"name":"Salary","type":"\"string\"","metadata":"{}"},{"name":"Date_of_Birth","type":"\"string\"","metadata":"{}"},{"name":"Age","type":"\"string\"","metadata":"{}"},{"name":"Country","type":"\"string\"","metadata":"{}"},{"name":"Department_id","type":"\"string\"","metadata":"{}"},{"name":"Date_of_Joining","type":"\"string\"","metadata":"{}"},{"name":"Manager_id","type":"\"string\"","metadata":"{}"},{"name":"Currency","type":"\"string\"","metadata":"{}"},{"name":"End_Date","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Employee_id</th><th>First_Name</th><th>Last_Name</th><th>Gender</th><th>Salary</th><th>Date_of_Birth</th><th>Age</th><th>Country</th><th>Department_id</th><th>Date_of_Joining</th><th>Manager_id</th><th>Currency</th><th>End_Date</th></tr></thead><tbody><tr><td>21</td><td>null</td><td>Onele</td><td>Female</td><td>96185</td><td>05/11/1990</td><td>-10</td><td>China</td><td>16</td><td>26/01/2014</td><td>9</td><td>AUD</td><td>18/12/2020</td></tr><tr><td>35</td><td>null</td><td>Ormshaw</td><td>Female</td><td>120118</td><td>06/10/1984</td><td>25</td><td>USA</td><td>16</td><td>21/09/2015</td><td>19</td><td>INR</td><td>07/01/2020</td></tr><tr><td>45</td><td>null</td><td>Passler</td><td>Female</td><td>77288</td><td>12/10/1982</td><td>-10</td><td>Japan</td><td>16</td><td>09/03/2013</td><td>17</td><td>INR</td><td>31/08/2019</td></tr><tr><td>47</td><td>null</td><td>Astbery</td><td>Male</td><td>64429</td><td>06/01/1992</td><td>52</td><td>Japan</td><td>16</td><td>11/06/2016</td><td>1</td><td>AUD</td><td>26/03/2020</td></tr><tr><td>49</td><td>null</td><td>McCloch</td><td>Male</td><td>95866</td><td>04/04/1989</td><td>41</td><td>USA</td><td>16</td><td>13/05/2014</td><td>17</td><td>INR</td><td>21/03/2020</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nSET spark.databricks.delta.retentionDurationCheck.enabled = false"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c94c06d2-3e3e-4061-b9a4-586a40015408"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[{"name":"_sqldf","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"key","nullable":false,"type":"string"},{"metadata":{},"name":"value","nullable":false,"type":"string"}],"type":"struct"},"tableIdentifier":null}],"data":[["spark.databricks.delta.retentionDurationCheck.enabled","false"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"key","type":"\"string\"","metadata":"{}"},{"name":"value","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>key</th><th>value</th></tr></thead><tbody><tr><td>spark.databricks.delta.retentionDurationCheck.enabled</td><td>false</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\n--vacuum is to prevent that user can go back to previous version. Once we set up with this, user won't go back to check any previous versions but the lastes version of data \nVACUUM DATAVOWELDB.employee_delta_table RETAIN 0 HOURS"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e1ee830-3441-42f5-a793-cf24ddba2e39"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[{"name":"_sqldf","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"path","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null}],"data":[["dbfs:/DATAVOWEL/DELTA/EMPLOYEE_DATA"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th></tr></thead><tbody><tr><td>dbfs:/DATAVOWEL/DELTA/EMPLOYEE_DATA</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nselect * from employee_delta_table version as of 2 where employee_id in (21,35,45,47,49)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eb1e8573-a58d-4a5e-807a-53c042e8c706"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[{"name":"_sqldf","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"Employee_id","nullable":true,"type":"string"},{"metadata":{},"name":"First_Name","nullable":true,"type":"string"},{"metadata":{},"name":"Last_Name","nullable":true,"type":"string"},{"metadata":{},"name":"Gender","nullable":true,"type":"string"},{"metadata":{},"name":"Salary","nullable":true,"type":"string"},{"metadata":{},"name":"Date_of_Birth","nullable":true,"type":"string"},{"metadata":{},"name":"Age","nullable":true,"type":"string"},{"metadata":{},"name":"Country","nullable":true,"type":"string"},{"metadata":{},"name":"Department_id","nullable":true,"type":"string"},{"metadata":{},"name":"Date_of_Joining","nullable":true,"type":"string"},{"metadata":{},"name":"Manager_id","nullable":true,"type":"string"},{"metadata":{},"name":"Currency","nullable":true,"type":"string"},{"metadata":{},"name":"End_Date","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null}],"data":[["21",null,"Onele","Female","96185","05/11/1990","-10","China","16","26/01/2014","9","AUD","18/12/2020"],["35",null,"Ormshaw","Female","120118","06/10/1984","25","USA","16","21/09/2015","19","INR","07/01/2020"],["45",null,"Passler","Female","77288","12/10/1982","-10","Japan","16","09/03/2013","17","INR","31/08/2019"],["47",null,"Astbery","Male","64429","06/01/1992","52","Japan","16","11/06/2016","1","AUD","26/03/2020"],["49",null,"McCloch","Male","95866","04/04/1989","41","USA","16","13/05/2014","17","INR","21/03/2020"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"Employee_id","type":"\"string\"","metadata":"{}"},{"name":"First_Name","type":"\"string\"","metadata":"{}"},{"name":"Last_Name","type":"\"string\"","metadata":"{}"},{"name":"Gender","type":"\"string\"","metadata":"{}"},{"name":"Salary","type":"\"string\"","metadata":"{}"},{"name":"Date_of_Birth","type":"\"string\"","metadata":"{}"},{"name":"Age","type":"\"string\"","metadata":"{}"},{"name":"Country","type":"\"string\"","metadata":"{}"},{"name":"Department_id","type":"\"string\"","metadata":"{}"},{"name":"Date_of_Joining","type":"\"string\"","metadata":"{}"},{"name":"Manager_id","type":"\"string\"","metadata":"{}"},{"name":"Currency","type":"\"string\"","metadata":"{}"},{"name":"End_Date","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Employee_id</th><th>First_Name</th><th>Last_Name</th><th>Gender</th><th>Salary</th><th>Date_of_Birth</th><th>Age</th><th>Country</th><th>Department_id</th><th>Date_of_Joining</th><th>Manager_id</th><th>Currency</th><th>End_Date</th></tr></thead><tbody><tr><td>21</td><td>null</td><td>Onele</td><td>Female</td><td>96185</td><td>05/11/1990</td><td>-10</td><td>China</td><td>16</td><td>26/01/2014</td><td>9</td><td>AUD</td><td>18/12/2020</td></tr><tr><td>35</td><td>null</td><td>Ormshaw</td><td>Female</td><td>120118</td><td>06/10/1984</td><td>25</td><td>USA</td><td>16</td><td>21/09/2015</td><td>19</td><td>INR</td><td>07/01/2020</td></tr><tr><td>45</td><td>null</td><td>Passler</td><td>Female</td><td>77288</td><td>12/10/1982</td><td>-10</td><td>Japan</td><td>16</td><td>09/03/2013</td><td>17</td><td>INR</td><td>31/08/2019</td></tr><tr><td>47</td><td>null</td><td>Astbery</td><td>Male</td><td>64429</td><td>06/01/1992</td><td>52</td><td>Japan</td><td>16</td><td>11/06/2016</td><td>1</td><td>AUD</td><td>26/03/2020</td></tr><tr><td>49</td><td>null</td><td>McCloch</td><td>Male</td><td>95866</td><td>04/04/1989</td><td>41</td><td>USA</td><td>16</td><td>13/05/2014</td><td>17</td><td>INR</td><td>21/03/2020</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nselect * from employee_delta_table version as of 1 where employee_id in (21,35,45,47,49)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"647b995c-06bc-467e-9bee-d77da4b72337"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 375.0 failed 4 times, most recent failure: Lost task 0.3 in stage 375.0 (TID 893, 10.139.64.4, executor driver): com.databricks.sql.io.FileReadException: Error while reading file dbfs:/DATAVOWEL/DELTA/EMPLOYEE_DATA/Department_id=16/part-00000-35dfea74-aea7-4eae-be6f-713c58c5f711.c000.snappy.parquet. A file referenced in the transaction log cannot be found. This occurs when data has been manually deleted from the file system rather than using the table `DELETE` statement. For more information, see https://docs.microsoft.com/azure/databricks/delta/delta-intro#frequently-asked-questions\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:349)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:307)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$prepareNextFile$1(FileScanRDD.scala:499)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:68)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:54)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:101)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.FileNotFoundException: dbfs:/DATAVOWEL/DELTA/EMPLOYEE_DATA/Department_id=16/part-00000-35dfea74-aea7-4eae-be6f-713c58c5f711.c000.snappy.parquet\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$2(DatabricksFileSystemV2.scala:775)\n\tat com.databricks.s3a.S3AExeceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:66)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$1(DatabricksFileSystemV2.scala:761)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:431)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:239)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:234)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:231)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:455)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:269)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:455)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:412)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:338)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:455)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:761)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:201)\n\tat com.databricks.spark.metrics.FileSystemWithMetrics.getFileStatus(FileSystemWithMetrics.scala:303)\n\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:39)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:452)\n\tat com.databricks.sql.io.parquet.CachingParquetFileReader.readFooter(CachingParquetFileReader.java:405)\n\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.prepare(SpecificParquetRecordReaderBase.java:128)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$1(ParquetFileFormat.scala:393)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:291)\n\t... 18 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2564)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2511)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2505)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2505)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1197)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1197)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1197)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2766)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2713)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2701)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:983)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2339)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:298)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:308)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:82)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:88)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:508)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollectResult(limit.scala:58)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:2994)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectResult$1(Dataset.scala:2985)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3709)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:142)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:275)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:127)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:845)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:225)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3707)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:2984)\n\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation0(OutputAggregator.scala:194)\n\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation(OutputAggregator.scala:57)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal.generateTableResult(PythonDriverLocal.scala:1158)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal.$anonfun$getResultBufferInternal$1(PythonDriverLocal.scala:1070)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal.withInterpLock(PythonDriverLocal.scala:857)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal.getResultBufferInternal(PythonDriverLocal.scala:939)\n\tat com.databricks.backend.daemon.driver.DriverLocal.getResultBuffer(DriverLocal.scala:556)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal.outputSuccess(PythonDriverLocal.scala:899)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal.$anonfun$repl$8(PythonDriverLocal.scala:384)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal.withInterpLock(PythonDriverLocal.scala:857)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal.repl(PythonDriverLocal.scala:371)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$10(DriverLocal.scala:449)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:239)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:234)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:231)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:47)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:269)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:47)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:426)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:653)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:645)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:486)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:598)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:391)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.sql.io.FileReadException: Error while reading file dbfs:/DATAVOWEL/DELTA/EMPLOYEE_DATA/Department_id=16/part-00000-35dfea74-aea7-4eae-be6f-713c58c5f711.c000.snappy.parquet. A file referenced in the transaction log cannot be found. This occurs when data has been manually deleted from the file system rather than using the table `DELETE` statement. For more information, see https://docs.microsoft.com/azure/databricks/delta/delta-intro#frequently-asked-questions\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:349)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:307)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$prepareNextFile$1(FileScanRDD.scala:499)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:68)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:54)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:101)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.io.FileNotFoundException: dbfs:/DATAVOWEL/DELTA/EMPLOYEE_DATA/Department_id=16/part-00000-35dfea74-aea7-4eae-be6f-713c58c5f711.c000.snappy.parquet\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$2(DatabricksFileSystemV2.scala:775)\n\tat com.databricks.s3a.S3AExeceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:66)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$1(DatabricksFileSystemV2.scala:761)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:431)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:239)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:234)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:231)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:455)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:269)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:455)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:412)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:338)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:455)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:761)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:201)\n\tat com.databricks.spark.metrics.FileSystemWithMetrics.getFileStatus(FileSystemWithMetrics.scala:303)\n\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:39)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:452)\n\tat com.databricks.sql.io.parquet.CachingParquetFileReader.readFooter(CachingParquetFileReader.java:405)\n\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.prepare(SpecificParquetRecordReaderBase.java:128)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$1(ParquetFileFormat.scala:393)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:291)\n\t... 18 more\n","errorSummary":"FileReadException: Error while reading file dbfs:/DATAVOWEL/DELTA/EMPLOYEE_DATA/Department_id=16/part-00000-35dfea74-aea7-4eae-be6f-713c58c5f711.c000.snappy.parquet. A file referenced in the transaction log cannot be found. This occurs when data has been manually deleted from the file system rather than using the table `DELETE` statement. For more information, see https://docs.microsoft.com/azure/databricks/delta/delta-intro#frequently-asked-questions\nCaused by: FileNotFoundException: dbfs:/DATAVOWEL/DELTA/EMPLOYEE_DATA/Department_id=16/part-00000-35dfea74-aea7-4eae-be6f-713c58c5f711.c000.snappy.parquet","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 375.0 failed 4 times, most recent failure: Lost task 0.3 in stage 375.0 (TID 893, 10.139.64.4, executor driver): com.databricks.sql.io.FileReadException: Error while reading file dbfs:/DATAVOWEL/DELTA/EMPLOYEE_DATA/Department_id=16/part-00000-35dfea74-aea7-4eae-be6f-713c58c5f711.c000.snappy.parquet. A file referenced in the transaction log cannot be found. This occurs when data has been manually deleted from the file system rather than using the table `DELETE` statement. For more information, see https://docs.microsoft.com/azure/databricks/delta/delta-intro#frequently-asked-questions\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:349)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:307)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$prepareNextFile$1(FileScanRDD.scala:499)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:68)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:54)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:101)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.FileNotFoundException: dbfs:/DATAVOWEL/DELTA/EMPLOYEE_DATA/Department_id=16/part-00000-35dfea74-aea7-4eae-be6f-713c58c5f711.c000.snappy.parquet\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$2(DatabricksFileSystemV2.scala:775)\n\tat com.databricks.s3a.S3AExeceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:66)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$1(DatabricksFileSystemV2.scala:761)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:431)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:239)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:234)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:231)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:455)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:269)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:455)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:412)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:338)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:455)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:761)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:201)\n\tat com.databricks.spark.metrics.FileSystemWithMetrics.getFileStatus(FileSystemWithMetrics.scala:303)\n\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:39)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:452)\n\tat com.databricks.sql.io.parquet.CachingParquetFileReader.readFooter(CachingParquetFileReader.java:405)\n\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.prepare(SpecificParquetRecordReaderBase.java:128)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$1(ParquetFileFormat.scala:393)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:291)\n\t... 18 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2564)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2511)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2505)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2505)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1197)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1197)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1197)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2766)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2713)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2701)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:983)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2339)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:298)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:308)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:82)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:88)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:508)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollectResult(limit.scala:58)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:2994)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectResult$1(Dataset.scala:2985)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3709)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:142)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:275)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:127)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:845)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:225)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3707)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:2984)\n\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation0(OutputAggregator.scala:194)\n\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation(OutputAggregator.scala:57)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal.generateTableResult(PythonDriverLocal.scala:1158)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal.$anonfun$getResultBufferInternal$1(PythonDriverLocal.scala:1070)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal.withInterpLock(PythonDriverLocal.scala:857)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal.getResultBufferInternal(PythonDriverLocal.scala:939)\n\tat com.databricks.backend.daemon.driver.DriverLocal.getResultBuffer(DriverLocal.scala:556)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal.outputSuccess(PythonDriverLocal.scala:899)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal.$anonfun$repl$8(PythonDriverLocal.scala:384)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal.withInterpLock(PythonDriverLocal.scala:857)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal.repl(PythonDriverLocal.scala:371)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$10(DriverLocal.scala:449)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:239)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:234)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:231)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:47)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:269)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:47)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:426)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:653)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:645)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:486)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:598)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:391)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.sql.io.FileReadException: Error while reading file dbfs:/DATAVOWEL/DELTA/EMPLOYEE_DATA/Department_id=16/part-00000-35dfea74-aea7-4eae-be6f-713c58c5f711.c000.snappy.parquet. A file referenced in the transaction log cannot be found. This occurs when data has been manually deleted from the file system rather than using the table `DELETE` statement. For more information, see https://docs.microsoft.com/azure/databricks/delta/delta-intro#frequently-asked-questions\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:349)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:307)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$prepareNextFile$1(FileScanRDD.scala:499)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:68)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:54)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:101)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.io.FileNotFoundException: dbfs:/DATAVOWEL/DELTA/EMPLOYEE_DATA/Department_id=16/part-00000-35dfea74-aea7-4eae-be6f-713c58c5f711.c000.snappy.parquet\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$2(DatabricksFileSystemV2.scala:775)\n\tat com.databricks.s3a.S3AExeceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:66)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$1(DatabricksFileSystemV2.scala:761)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:431)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:239)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:234)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:231)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:455)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:269)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:455)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:412)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:338)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:455)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:761)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:201)\n\tat com.databricks.spark.metrics.FileSystemWithMetrics.getFileStatus(FileSystemWithMetrics.scala:303)\n\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:39)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:452)\n\tat com.databricks.sql.io.parquet.CachingParquetFileReader.readFooter(CachingParquetFileReader.java:405)\n\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.prepare(SpecificParquetRecordReaderBase.java:128)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$1(ParquetFileFormat.scala:393)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:291)\n\t... 18 more"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c9f30839-8333-43fd-a4c7-c94821cd239a"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"DeltaTableProcessing","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":4125592599887388,"dataframes":["_sqldf"]}},"language":"python","widgets":{},"notebookOrigID":4125592599887365}},"nbformat":4,"nbformat_minor":0}
